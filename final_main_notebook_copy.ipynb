{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tools.dataset import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import NoiseFactory as NF\n",
    "import TransformationFactory as TF\n",
    "from resnet20 import ResNetCIFAR\n",
    "\n",
    "import test_dataset as test_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Resnet-20 Class & Instantiation Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def instantiate_ResNet_model():\n",
    "    # specify the device for computation\n",
    "    #############################################\n",
    "    # your code here\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    if device =='cuda':\n",
    "        print(\"Run on GPU...\")\n",
    "    else:\n",
    "        print(\"Run on CPU...\")\n",
    "\n",
    "    # Model Definition\n",
    "    net = ResNetCIFAR(num_layers=20, Nbits=None) # from HW 4\n",
    "    net = net.to(device)\n",
    "    #############################################\n",
    "    return net, device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def set_up_loss_function(device,\n",
    "                         net,\n",
    "                         INITIAL_LR: float=0.01,\n",
    "                         MOMENTUM: float=0.9,\n",
    "                         regularization_method: str=\"L2\",\n",
    "                         REG: float=0.0001):\n",
    "    # hyperparameters, do NOT change right now\n",
    "    # initial learning rate\n",
    "    INITIAL_LR = INITIAL_LR\n",
    "\n",
    "    # momentum for optimizer\n",
    "    MOMENTUM = MOMENTUM\n",
    "\n",
    "    regularization_method = regularization_method\n",
    "    # L2 regularization strength\n",
    "    REG = REG                  # <--- used to control both L2 and L1 regularization methods\n",
    "\n",
    "    #############################################\n",
    "    # your code here\n",
    "    # create loss function\n",
    "    # add L1 regularization here.\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Add optimizer\n",
    "    if regularization_method == \"L2\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n",
    "    else:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM)\n",
    "    #############################################\n",
    "\n",
    "    return criterion, optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Data Transformation and Data Loader Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_training_transforms(intensity:int=0, noise = None, p:float=0.5):\n",
    "    if intensity is None:\n",
    "        intensity = 0\n",
    "\n",
    "    n_factory = NF.NoiseFactory()\n",
    "    transforms_factory = TF.TransformationFactory()\n",
    "\n",
    "    transform_train = transforms_factory.get_transformation_composition_by_name(transformation_set=noise, intensity=intensity, p=p, train_test_validation='train')\n",
    "    noise_factory_function = n_factory.get_noise_by_name(noise_type_name=noise, intensity=intensity, p=p)\n",
    "    transform_val =transforms_factory.get_transformation_composition_by_name(transformation_set=None, p=p, train_test_validation='val')\n",
    "\n",
    "    return transform_train, transform_val, noise_factory_function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def build_train_validation_sets(TRAIN_BATCH_SIZE: int =128,\n",
    "                                VAL_BATCH_SIZE: int = 100,\n",
    "                                training_noise_type: str = None,\n",
    "                                training_noise_type_intensity: int = None,\n",
    "                                probability_of_being_applied: float = 0.5):\n",
    "\n",
    "    # set transformations\n",
    "    transform_train, transform_val, noise_factory_function = get_training_transforms(noise=training_noise_type,\n",
    "                                                                                     intensity=training_noise_type_intensity,\n",
    "                                                                                     p=probability_of_being_applied)\n",
    "\n",
    "    # a few arguments, do NOT change these\n",
    "    DATA_ROOT = \"./data\"\n",
    "    TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "    VAL_BATCH_SIZE = VAL_BATCH_SIZE\n",
    "\n",
    "    #############################################\n",
    "    # your code here\n",
    "    # construct dataset\n",
    "    train_set = test_ds.CIFAR10(root='./data', train=True, download=True, transform=transform_train, imgaug_types=noise_factory_function)\n",
    "    # train_set = CIFAR10(\n",
    "    #     root=DATA_ROOT,\n",
    "    #     mode='train',\n",
    "    #     download=True,\n",
    "    #     transform=transform_train,    # your code\n",
    "    #     imgaug_types=noise_factory_function\n",
    "    # )\n",
    "    val_set = CIFAR10(\n",
    "        root=DATA_ROOT,\n",
    "        mode='val',\n",
    "        download=True,\n",
    "        transform=transform_val    # your code\n",
    "    )\n",
    "\n",
    "    # construct dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=TRAIN_BATCH_SIZE ,  # your code\n",
    "        shuffle=True,     # your code\n",
    "        num_workers=4\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=VAL_BATCH_SIZE,  # your code\n",
    "        shuffle=True,     # your code, shuffle to increase the random order the images are presented to prevent the model from experinceing cycles (which will make it harder to converge)\n",
    "        num_workers=4\n",
    "    )\n",
    "    #############################################\n",
    "\n",
    "    return train_set, val_set, train_loader, val_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Train/Validation function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def resnet_train_model(TRAIN_BATCH_SIZE: int = 128, # agnostic to transformations\n",
    "                       VAL_BATCH_SIZE: int = 100,\n",
    "                       training_noise_type: str = None,\n",
    "                       training_noise_type_intensity: int = 0,\n",
    "                       probability_of_being_applied: float = 0.5,\n",
    "                       training_epochs: int = 200,\n",
    "                       model_name: str = \"NO_NOISE_MODEL.pt\",\n",
    "                       INITIAL_LR: float = 0.01,\n",
    "                       MOMENTUM: float = 0.9,\n",
    "                       regularization_method: str = \"L2\",\n",
    "                       REG: float = 0.0001\n",
    "                       ):\n",
    "\n",
    "    # 2. Build/load training/validation data sets\n",
    "    #    This injects noise into the training data set\n",
    "    train_set, val_set, train_loader, val_loader = build_train_validation_sets(TRAIN_BATCH_SIZE=TRAIN_BATCH_SIZE,\n",
    "                                                                               VAL_BATCH_SIZE=VAL_BATCH_SIZE,\n",
    "                                                                               training_noise_type=training_noise_type,\n",
    "                                                                               training_noise_type_intensity=training_noise_type_intensity,\n",
    "                                                                               probability_of_being_applied = probability_of_being_applied)\n",
    "\n",
    "    # 3.A. instantiate model, and put it on a GPU if a GPU is available\n",
    "    net, device = instantiate_ResNet_model()\n",
    "\n",
    "    # 3.B. set-up criterion, and optimizer\n",
    "    criterion, optimizer = set_up_loss_function(device=device,\n",
    "                                                net = net,\n",
    "                                                INITIAL_LR=INITIAL_LR,\n",
    "                                                MOMENTUM=MOMENTUM,\n",
    "                                                regularization_method=regularization_method,\n",
    "                                                REG=REG)\n",
    "\n",
    "    # the folder where the trained model is saved\n",
    "    CHECKPOINT_FOLDER = \"./saved_models\"\n",
    "\n",
    "    #\n",
    "    # Code to train the model goes here\n",
    "    #      All code from a training block in HW2/HW5 would go here\n",
    "    current_lr = INITIAL_LR\n",
    "    best_validation_acc = 0\n",
    "    print(\"==> Training starts!\")\n",
    "    print(\"=\"*50)\n",
    "    vald_acc = []\n",
    "    lr_reset_min = 20\n",
    "    initial_lr = True\n",
    "    vald_loss = []\n",
    "\n",
    "    for i in range(0, training_epochs):\n",
    "        # handle the learning rate scheduler.\n",
    "        lr_reset_min -= 1\n",
    "        if lr_reset_min<=0:\n",
    "            if initial_lr:\n",
    "                current_lr=0.1\n",
    "                lr_reset_min = 40\n",
    "                initial_lr = False\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = current_lr\n",
    "                    print(\"Current learning rate has decayed to %f\" %current_lr)\n",
    "            elif abs(vald_acc[i-1] - np.mean(vald_acc[i-9:i]))*100 < 5:\n",
    "                current_lr = current_lr * 0.1\n",
    "                lr_reset_min = 60\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = current_lr\n",
    "                    print(\"Current learning rate has decayed to %f\" %current_lr)\n",
    "        ######################\n",
    "        # switch to train mode\n",
    "        net.train()\n",
    "        ######################\n",
    "        print(\"Epoch %d:\" %i)\n",
    "        total_examples = 0 # this helps you compute the training accuracy\n",
    "        correct_examples = 0\n",
    "        train_loss = 0 # track training loss if you want\n",
    "\n",
    "        # Train the model for 1 epoch.\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader): # this might really need to be changed over to something new\n",
    "            # copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # compute the output and loss\n",
    "            outputs = net(inputs)\n",
    "            # loss = criterion(outputs, targets)\n",
    "            if regularization_method == \"L1\":\n",
    "                L1_sum = 0.0\n",
    "                for name, module in net.named_modules():\n",
    "                    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                        L1_sum = L1_sum + torch.sum(torch.abs(module.weight))\n",
    "                loss = criterion(outputs, targets) + REG * L1_sum\n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            # zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # apply gradient and update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # count the number of correctly predicted samples in the current batch\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = predicted.eq(targets).sum()\n",
    "            total_examples += targets.shape[0]\n",
    "            train_loss  += loss\n",
    "            correct_examples += correct.item()\n",
    "            ####################################\n",
    "\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        avg_acc = correct_examples / total_examples\n",
    "        print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "\n",
    "        # ######################\n",
    "        # # switch to val mode\n",
    "        # net.eval()\n",
    "        # ######################\n",
    "        # total_examples = 0\n",
    "        # correct_examples = 0\n",
    "        #\n",
    "        # val_loss = 0 # again, track the validation loss if you want\n",
    "        #\n",
    "        # # disable gradient during validation, which can save GPU memory\n",
    "        # with torch.no_grad():\n",
    "        #     for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        #         ####################################\n",
    "        #         # your code here\n",
    "        #         # copy inputs to device\n",
    "        #         inputs = inputs.to(device)\n",
    "        #         targets = targets.to(device)\n",
    "        #\n",
    "        #         # compute the output and loss\n",
    "        #         outputs = net(inputs)\n",
    "        #         loss = criterion(outputs, targets)\n",
    "        #\n",
    "        #         # count the number of correctly predicted samples in the current batch\n",
    "        #         _, predicted = torch.max(outputs, 1)\n",
    "        #         correct = predicted.eq(targets).sum()\n",
    "        #         total_examples += targets.shape[0]\n",
    "        #         val_loss  += loss\n",
    "        #         correct_examples += correct.item()\n",
    "        #         ####################################\n",
    "\n",
    "        # avg_loss = val_loss / len(val_loader)\n",
    "        # avg_acc = correct_examples / total_examples\n",
    "        # print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "        vald_acc.append(avg_acc)\n",
    "        vald_loss.append(float(avg_loss))\n",
    "\n",
    "        # Make sure to save the best model checkpoint for this noise type\n",
    "        if avg_acc > best_validation_acc:\n",
    "            best_validation_acc = avg_acc\n",
    "            if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "                os.makedirs(CHECKPOINT_FOLDER)\n",
    "            print(\"Saving ...\")\n",
    "            state = {'state_dict': net.state_dict(),\n",
    "                     'epoch': i,\n",
    "                     'lr': current_lr}\n",
    "            torch.save(state, os.path.join(CHECKPOINT_FOLDER, model_name))\n",
    "\n",
    "    #training time data\n",
    "    training_time_file_name = CHECKPOINT_FOLDER + '/' + model_name + '_training_time_data.csv'\n",
    "    valdation_df = list(zip(vald_loss, vald_acc))\n",
    "    df = pd.DataFrame(valdation_df, columns = ['Loss', 'Accuracy'])\n",
    "    df.to_csv(path_or_buf=training_time_file_name, sep=',',)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taylor/Duke/ECE661/projects/untitled/final_project/TransformationFactory.py:140: UserWarning: TransformationsFactory not supplied with an expected transformation set name, so standard set supplied.\n",
      "  warnings.warn(\"TransformationsFactory not supplied with an expected transformation set name, so standard set supplied.\")\n",
      "/home/taylor/Duke/ECE661/projects/untitled/final_project/NoiseFactory.py:48: UserWarning: NoiseFactory not supplied with an expected name, so noNoise supplied.\n",
      "  warnings.warn(\"NoiseFactory not supplied with an expected name, so noNoise supplied.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/cifar10_trainval_F22.zip\n",
      "Extracting ./data/cifar10_trainval_F22.zip to ./data\n",
      "Files already downloaded and verified\n",
      "Run on GPU...\n",
      "==> Training starts!\n",
      "==================================================\n",
      "Epoch 0:\n",
      "Training loss: 1.6016, Training accuracy: 0.4097\n",
      "Saving ...\n",
      "Epoch 1:\n",
      "Training loss: 1.1849, Training accuracy: 0.5708\n",
      "Saving ...\n",
      "Epoch 2:\n",
      "Training loss: 0.9819, Training accuracy: 0.6483\n",
      "Saving ...\n",
      "Epoch 3:\n",
      "Training loss: 0.8557, Training accuracy: 0.6962\n",
      "Saving ...\n",
      "Epoch 4:\n",
      "Training loss: 0.7674, Training accuracy: 0.7282\n",
      "Saving ...\n",
      "Epoch 5:\n",
      "Training loss: 0.7031, Training accuracy: 0.7531\n",
      "Saving ...\n",
      "Epoch 6:\n",
      "Training loss: 0.6549, Training accuracy: 0.7706\n",
      "Saving ...\n",
      "Epoch 7:\n",
      "Training loss: 0.6191, Training accuracy: 0.7840\n",
      "Saving ...\n",
      "Epoch 8:\n",
      "Training loss: 0.5807, Training accuracy: 0.7972\n",
      "Saving ...\n",
      "Epoch 9:\n",
      "Training loss: 0.5543, Training accuracy: 0.8073\n",
      "Saving ...\n",
      "Epoch 10:\n",
      "Training loss: 0.5259, Training accuracy: 0.8168\n",
      "Saving ...\n",
      "Epoch 11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noise_type = [\"standard\"] #[\"motion-blur\", \"random-perspective\", \"random-resize-and-crop\", \"random-resize-and-crop\", \"gaussian-blur\", \"scale\", \"pepper\", \"pepper\"]\n",
    "intensity =  [0]#[ 1, 4, 3, 3, 1, 4, 4, 2] #5, 2 ]\n",
    "frequency =  [1.0] #[0.3, 0.2, 0.9, 0.7, 0.6, 0.7, 0.1, 0.3]\n",
    "model_name = [\"base_model.pth\"]#[\"DoE1_model_10.pth\", \"DoE1_model_11.pth\", \"DoE1_model_12.pth\", \"DoE1_model_13.pth\", \"DoE1_model_14.pth\", \"DoE1_model_15.pth\", \"DoE1_model_16.pth\", \"DoE1_model_17.pth\", \"DoE1_model_18.pth\", \"DoE1_model_19.pth\" ]\n",
    "for i in range (0, len(noise_type)):\n",
    "    resnet_train_model(\n",
    "        training_noise_type = noise_type[i],\n",
    "        training_noise_type_intensity = intensity[i],\n",
    "        probability_of_being_applied= frequency[i],\n",
    "        model_name = model_name[i], training_epochs=200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Test Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_test_transforms(intensity:int=0, noise = None, p:float=0.5):\n",
    "    if intensity is None:\n",
    "        intensity = 0\n",
    "\n",
    "    n_factory = NF.NoiseFactory()\n",
    "    transforms_factory = TF.TransformationFactory()\n",
    "\n",
    "    transform_test = transforms_factory.get_transformation_composition_by_name(transformation_set=noise, intensity=intensity, p=p, train_test_validation='test')\n",
    "    noise_factory_function = n_factory.get_noise_by_name(noise_type_name=noise, intensity=intensity, p=p)\n",
    "\n",
    "    return transform_test, noise_factory_function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_test_data(test_noise_transformation: str = None,\n",
    "                  test_noise_transformation_intensity: int = None):\n",
    "    DATA_ROOT = \"./data\"\n",
    "    transform_test, noise_factory_function = get_test_transforms(intensity=test_noise_transformation_intensity, noise = test_noise_transformation, p=1.0)\n",
    "    test_set = CIFAR10(\n",
    "        root=DATA_ROOT,\n",
    "        mode='test',\n",
    "        download=True,\n",
    "        transform=transform_test,\n",
    "        imgaug_types=noise_factory_function\n",
    "    )\n",
    "\n",
    "    return test_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def test(net):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\n",
    "    ])\n",
    "\n",
    "    testset = test_ds.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=1)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test accuracy=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "    return val_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9654, Test accuracy=0.6739\n",
      "{'resnetnn.pth': {'scale': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739], 'salt': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739], 'gaussian-blur': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739], 'pepper': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739], 'coarse-pepper': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739], 'motion-blur': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739], 'random-resized-crop': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739], 'random-perspective': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'resnetnn.pth': {'scale': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739],\n  'salt': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739],\n  'gaussian-blur': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739],\n  'pepper': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739],\n  'coarse-pepper': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739],\n  'motion-blur': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739],\n  'random-resized-crop': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739],\n  'random-perspective': [0.6739, 0.6739, 0.6739, 0.6739, 0.6739]}}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_function(model_to_test: str = 'resnet',\n",
    "                  BATCH_SIZE:int = 100,\n",
    "                  test_noise_transformation: str = None,\n",
    "                  test_noise_transformation_intensity: int = None,\n",
    "                  ):\n",
    "    model_path = f'./saved_models/{model_to_test}'\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "    # # inject noise into the test data\n",
    "    # test_set = get_test_data(test_noise_transformation=test_noise_transformation,\n",
    "    #                          test_noise_transformation_intensity=test_noise_transformation_intensity)\n",
    "    #\n",
    "    # # do NOT shuffle your test data loader!!!!!!!!!!!!!!!!\n",
    "    # # otherwise the order of samples will be messed up\n",
    "    # # and your test accuracy is likely to drop to random guessing level\n",
    "    # test_loader = DataLoader(\n",
    "    #     test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    #########################################################\n",
    "    # use your model to generate predictions on test data\n",
    "    # and save the results into variable \"results\"\n",
    "    # \"results\" should be either a numpy array or a torch tensor with length of 10000\n",
    "\n",
    "    # initialize a resnet and load trained weights\n",
    "    # net, device = instantiate_ResNet_model()\n",
    "    # checkpoint = torch.load(model_path) # change the path to your own checkpoint file\n",
    "    # net.load_state_dict(checkpoint['state_dict'])\n",
    "    # net.cuda()\n",
    "\n",
    "    net = ResNetCIFAR()\n",
    "    checkpoint = torch.load('./saved_models/base_model.pth') # change the path to your own checkpoint file\n",
    "    net.load_state_dict(checkpoint['state_dict'])\n",
    "    net.cuda()\n",
    "\n",
    "    acc = test(net)\n",
    "\n",
    "\n",
    "    return acc # this should be a single accuracy number\n",
    "\n",
    "def run_testing():\n",
    "    # net, device = instantiate_ResNet_model()\n",
    "    list_of_model_names = [\"resnetnn.pth\"]\n",
    "    adversarial_noises = [\"scale\", \"salt\", \"gaussian-blur\", \"pepper\", \"coarse-pepper\", \"motion-blur\", \"random-resized-crop\",  \"random-perspective\"]\n",
    "    intensity_levels = [1, 2, 3, 4, 5]\n",
    "    noises_dict = {}\n",
    "    models_dict = {}\n",
    "\n",
    "    #\n",
    "    # This loop creates a dictionary of dictionaries that contains lists (i.e. the outer most dictionary contains entries for each model, and each of those entries is it self a dictionary that contsin entries for each adversarial noise type. Each adversarial noise type entry is a list, and those lists contain the accuracy results of the test in order from least intense to most intense)\n",
    "    #\n",
    "    for model in list_of_model_names:\n",
    "        for noise_type in adversarial_noises:\n",
    "            results = []\n",
    "            for intensity_level in intensity_levels:\n",
    "                result = test_function(model_to_test = model,\n",
    "                                       BATCH_SIZE = 100,\n",
    "                                       test_noise_transformation=noise_type,\n",
    "                                       test_noise_transformation_intensity=intensity_level)\n",
    "                results.append(result)\n",
    "            noises_dict[noise_type] = results\n",
    "        models_dict[model] = noises_dict\n",
    "    print(models_dict)\n",
    "    return models_dict\n",
    "\n",
    "    # The models_dict gets fed to the heatmap plotting function\n",
    "\n",
    "run_testing()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Plotting Devices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Plotting Devices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
